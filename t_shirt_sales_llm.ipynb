{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff82c00",
   "metadata": {},
   "source": [
    "#### Google Palm LLM & API key setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297db3d6-c97c-4999-a16f-6b1e2e719516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping google-generativeai as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-ai-generativelanguage==0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.7.0) (2.41.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.7.0) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.7.0) (5.29.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage==0.7.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage==0.7.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage==0.7.0) (4.9.1)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage==0.7.0) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage==0.7.0) (0.6.1)\n",
      "Requirement already satisfied: langchain-google-genai in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.12)\n",
      "Requirement already satisfied: langchain-core>=0.3.75 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-google-genai) (0.3.75)\n",
      "Requirement already satisfied: google-ai-generativelanguage<1,>=0.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-google-genai) (2.11.9)\n",
      "Requirement already satisfied: filetype<2,>=1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.41.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core>=0.3.75->langchain-google-genai) (0.4.25)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core>=0.3.75->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core>=0.3.75->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core>=0.3.75->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core>=0.3.75->langchain-google-genai) (24.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.75->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y google-generativeai\n",
    "!pip install --upgrade google-ai-generativelanguage==0.7.0\n",
    "!pip install --upgrade langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfed644f-5b28-4bfe-a533-d343e7a432cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLOW world \n"
     ]
    }
   ],
   "source": [
    "print(\"HELLOW world \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e7496d7-3abf-4b7f-af3c-139433545ff5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-2.5-pro-preview-03-25',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-03-25',\n",
      "      display_name='Gemini 2.5 Pro Preview 03-25',\n",
      "      description='Gemini 2.5 Pro Preview 03-25',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-05-20',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash',\n",
      "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
      "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-17',\n",
      "      display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
      "      description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-05-06',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-06',\n",
      "      display_name='Gemini 2.5 Pro Preview 05-06',\n",
      "      description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-06-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-05',\n",
      "      display_name='Gemini 2.5 Pro Preview',\n",
      "      description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro',\n",
      "      base_model_id='',\n",
      "      version='2.5',\n",
      "      display_name='Gemini 2.5 Pro',\n",
      "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-exp',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Experimental',\n",
      "      description='Gemini 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash',\n",
      "      description='Gemini 2.0 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
      "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite 001',\n",
      "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite',\n",
      "      description='Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Preview Image Generation',\n",
      "      description='Gemini 2.0 Flash Preview Image Generation',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite-preview',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-pro-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1206',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview TTS',\n",
      "      description='Gemini 2.5 Flash Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Pro Preview TTS',\n",
      "      description='Gemini 2.5 Pro Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/learnlm-2.0-flash-experimental',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='LearnLM 2.0 Flash Experimental',\n",
      "      description='LearnLM 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=32768,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-1b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 1B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 4B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-12b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 12B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-27b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 27B',\n",
      "      description='',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3n-e4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E4B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3n-e2b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E2B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='Gemini Flash Latest',\n",
      "      display_name='Gemini Flash Latest',\n",
      "      description='Latest release of Gemini Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-flash-lite-latest',\n",
      "      base_model_id='',\n",
      "      version='Gemini Flash-Lite Latest',\n",
      "      display_name='Gemini Flash-Lite Latest',\n",
      "      description='Latest release of Gemini Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='Gemini Pro Latest',\n",
      "      display_name='Gemini Pro Latest',\n",
      "      description='Latest release of Gemini Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-lite',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash-Lite',\n",
      "      description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-image-preview',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Nano Banana',\n",
      "      description='Gemini 2.5 Flash Preview Image',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=1.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
      "      base_model_id='',\n",
      "      version='Gemini 2.5 Flash Preview 09-2025',\n",
      "      display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
      "      description='Gemini 2.5 Flash Preview Sep 2025',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-09-25',\n",
      "      display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
      "      description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-robotics-er-1.5-preview',\n",
      "      base_model_id='',\n",
      "      version='1.5-preview',\n",
      "      display_name='Gemini Robotics-ER 1.5 Preview',\n",
      "      description='Gemini Robotics-ER 1.5 Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-exp-03-07',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental 03-07',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-exp',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n",
      "Model(name='models/imagen-3.0-generate-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Imagen 3.0',\n",
      "      description='Vertex served Imagen 3.0 002 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 (Preview)',\n",
      "      description='Vertex served Imagen 4.0 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 Ultra (Preview)',\n",
      "      description='Vertex served Imagen 4.0 ultra model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-generate-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Imagen 4',\n",
      "      description='Vertex served Imagen 4.0 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-ultra-generate-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Imagen 4 Ultra',\n",
      "      description='Vertex served Imagen 4.0 ultra model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-fast-generate-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Imagen 4 Fast',\n",
      "      description='Vertex served Imagen 4.0 Fast model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-2.0-generate-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Veo 2',\n",
      "      description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
      "                   'enabled on the associated Google Cloud Platform account. Please visit '\n",
      "                   'https://console.cloud.google.com/billing to enable it.'),\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-3.0-generate-preview',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3',\n",
      "      description='Veo 3 preview.',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-3.0-fast-generate-preview',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3 fast',\n",
      "      description='Veo 3 fast preview.',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-3.0-generate-001',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3',\n",
      "      description='Veo 3',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-3.0-fast-generate-001',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3 fast',\n",
      "      description='Veo 3 fast',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-live-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description='Gemini 2.0 Flash 001',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-live-2.5-flash-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Live 2.5 Flash Preview',\n",
      "      description='Gemini Live 2.5 Flash Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-live-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash Live Preview',\n",
      "      description='Gemini 2.5 Flash Live Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-native-audio-latest',\n",
      "      base_model_id='',\n",
      "      version='Gemini 2.5 Flash Native Audio Latest',\n",
      "      display_name='Gemini 2.5 Flash Native Audio Latest',\n",
      "      description='Latest release of Gemini 2.5 Flash Native Audio',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-native-audio-preview-09-2025',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
      "      description='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n"
     ]
    }
   ],
   "source": [
    "from google import generativeai\n",
    "\n",
    "generativeai.configure(api_key=\"AIzaSyBE8ef6TzJGoiFjLrQwFjemWuBy-dUKk_4\")\n",
    "\n",
    "# Convert generator to list\n",
    "models = list(generativeai.list_models())\n",
    "\n",
    "# Print the models nicely\n",
    "for m in models:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de1952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Of course! Here is a simple way to explain oxidation to a 7-year-old.\\n\\n***\\n\\nImagine the air all around us has tiny, invisible things in it called **oxygen**. We need oxygen to breathe, and it\\'s super important!\\n\\n**Oxidation** is what happens when this oxygen from the air touches something and starts to change it.\\n\\nThink of oxygen as being a little bit grabby. When it touches certain things, it gives them a big, long \"hug\" and doesn\\'t let go. This hug changes the thing forever!\\n\\nHere are some examples you\\'ve probably seen:\\n\\n### 1. The Sad, Brown Apple\\n\\nHave you ever seen your mom or dad cut an apple, and after a little while, the inside turns brown?\\n\\n*   When you cut the apple, the fresh, white part is suddenly touching the air.\\n*   The **oxygen** in the air gives the apple a big hug!\\n*   This \"oxygen hug\" changes the apple and makes it turn brown.\\n*   That browning is **oxidation**!', \"\\n\\n### 2. The Rusty Old Bike\\n\\nThink about a shiny metal bike or scooter left outside in the rain. After a long time, it might get reddish-brown, crumbly spots on it. That's rust!\\n\\n*   The **oxygen** in the air and water gives the metal a big hug.\\n*   This hug changes the strong, shiny metal into weak, brown rust.\\n*   Rusting is a type of **oxidation**!\", '\\n\\n### 3. A Crackling Campfire\\n\\nThis one is a super-fast and hot version of oxidation!\\n\\n*   When you burn wood, the **oxygen** is hugging the wood *really, really fast* and with a lot of energy.\\n*   This super-fast hug creates fire, light, and heat!\\n*   Burning is a very fast kind of **oxidation**.', '\\n\\n### So, to put it simply:\\n\\n**Oxidation is just a fancy word for what happens when oxygen from the air changes something, like turning an apple brown or making metal rusty.**\\n\\nIt can be slow like rust, or super-fast like fire']\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-2.5-pro\",  # Use a valid model from your list\n",
    "    api_key=\"AIzaSyBE8ef6TzJGoiFjLrQwFjemWuBy-dUKk_4\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"what is oxidation define and explain to 7 year old student  \")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0d700",
   "metadata": {},
   "source": [
    "#### Connect with database and ask some basic questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adba66ba-c49c-452a-93ec-c654162716c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.43)\n",
      "Requirement already satisfied: langchain in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.29)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy) (4.15.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.4.25)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (3.11.18)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Collecting langchain-experimental\n",
      "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-experimental) (0.3.29)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-experimental) (0.3.75)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2.32.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.25)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.11.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (24.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.32.5->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.14.0)\n",
      "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
      "Installing collected packages: langchain-experimental\n",
      "Successfully installed langchain-experimental-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql sqlalchemy langchain langchain-community\n",
    "!pip install langchain-experimental\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e01456-43b3-4eac-9cd5-44f50e79e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE discounts (\n",
      "\tdiscount_id INTEGER NOT NULL AUTO_INCREMENT, \n",
      "\tt_shirt_id INTEGER NOT NULL, \n",
      "\tpct_discount DECIMAL(5, 2), \n",
      "\tPRIMARY KEY (discount_id), \n",
      "\tCONSTRAINT discounts_ibfk_1 FOREIGN KEY(t_shirt_id) REFERENCES t_shirts (t_shirt_id), \n",
      "\tCONSTRAINT discounts_chk_1 CHECK ((`pct_discount` between 0 and 100))\n",
      ")COLLATE utf8mb4_0900_ai_ci ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n",
      "\n",
      "/*\n",
      "3 rows from discounts table:\n",
      "discount_id\tt_shirt_id\tpct_discount\n",
      "1\t1\t10.00\n",
      "2\t2\t15.00\n",
      "3\t3\t20.00\n",
      "*/\n",
      "\n",
      "\n",
      "CREATE TABLE t_shirts (\n",
      "\tt_shirt_id INTEGER NOT NULL AUTO_INCREMENT, \n",
      "\tbrand ENUM('Van Huesen','Levi','Nike','Adidas') NOT NULL, \n",
      "\tcolor ENUM('Red','Blue','Black','White') NOT NULL, \n",
      "\tsize ENUM('XS','S','M','L','XL') NOT NULL, \n",
      "\tprice INTEGER, \n",
      "\tstock_quantity INTEGER NOT NULL, \n",
      "\tPRIMARY KEY (t_shirt_id), \n",
      "\tCONSTRAINT t_shirts_chk_1 CHECK ((`price` between 10 and 50))\n",
      ")COLLATE utf8mb4_0900_ai_ci ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n",
      "\n",
      "/*\n",
      "3 rows from t_shirts table:\n",
      "t_shirt_id\tbrand\tcolor\tsize\tprice\tstock_quantity\n",
      "1\tLevi\tWhite\tXS\t22\t100\n",
      "2\tVan Huesen\tBlue\tL\t32\t52\n",
      "3\tNike\tRed\tXL\t10\t34\n",
      "*/\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many t-shirts do we have left for Nike in extra small size and white color?\n",
      "SQLQuery:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_14944\\3827172522.py:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  qns1 = db_chain.run(\"How many t-shirts do we have left for Nike in extra small size and white color?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mQuestion: How many t-shirts do we have left for Nike in extra small size and white color?\n",
      "SQLQuery: SELECT `stock_quantity` FROM `t_shirts` WHERE `brand` = 'Nike' AND `size` = 'XS' AND `color` = 'White'\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(77,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mWe have 77 t-shirts left for Nike in extra small size and white color.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "We have 77 t-shirts left for Nike in extra small size and white color.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_experimental.sql.base import SQLDatabaseChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# MySQL connection parameters\n",
    "db_user = \"root\"\n",
    "db_password = \"root\"\n",
    "db_host = \"localhost\"\n",
    "db_name = \"atliq_tshirts\"\n",
    "\n",
    "# Create the SQLDatabase object\n",
    "db = SQLDatabase.from_uri(\n",
    "    f\"mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}\",\n",
    "    sample_rows_in_table_info=3\n",
    ")\n",
    "\n",
    "# Print schema info (optional, useful for debugging)\n",
    "print(db.table_info)\n",
    "\n",
    "#  Use Gemini instead of OpenAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-2.5-pro\",   # You can also try gemini-1.5-flash for faster\n",
    "    api_key=\"AIzaSyBE8ef6TzJGoiFjLrQwFjemWuBy-dUKk_4\",   # replace with your Gemini API key\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Build the SQL chain\n",
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "\n",
    "# Ask a question\n",
    "qns1 = db_chain.run(\"How many t-shirts do we have left for Nike in extra small size and white color?\")\n",
    "print(qns1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f9ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE discounts (\n",
      "\tdiscount_id INTEGER NOT NULL AUTO_INCREMENT, \n",
      "\tt_shirt_id INTEGER NOT NULL, \n",
      "\tpct_discount DECIMAL(5, 2), \n",
      "\tPRIMARY KEY (discount_id), \n",
      "\tCONSTRAINT discounts_ibfk_1 FOREIGN KEY(t_shirt_id) REFERENCES t_shirts (t_shirt_id), \n",
      "\tCONSTRAINT discounts_chk_1 CHECK ((`pct_discount` between 0 and 100))\n",
      ")COLLATE utf8mb4_0900_ai_ci ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n",
      "\n",
      "/*\n",
      "3 rows from discounts table:\n",
      "discount_id\tt_shirt_id\tpct_discount\n",
      "1\t1\t10.00\n",
      "2\t2\t15.00\n",
      "3\t3\t20.00\n",
      "*/\n",
      "\n",
      "\n",
      "CREATE TABLE t_shirts (\n",
      "\tt_shirt_id INTEGER NOT NULL AUTO_INCREMENT, \n",
      "\tbrand ENUM('Van Huesen','Levi','Nike','Adidas') NOT NULL, \n",
      "\tcolor ENUM('Red','Blue','Black','White') NOT NULL, \n",
      "\tsize ENUM('XS','S','M','L','XL') NOT NULL, \n",
      "\tprice INTEGER, \n",
      "\tstock_quantity INTEGER NOT NULL, \n",
      "\tPRIMARY KEY (t_shirt_id), \n",
      "\tCONSTRAINT t_shirts_chk_1 CHECK ((`price` between 10 and 50))\n",
      ")COLLATE utf8mb4_0900_ai_ci ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n",
      "\n",
      "/*\n",
      "3 rows from t_shirts table:\n",
      "t_shirt_id\tbrand\tcolor\tsize\tprice\tstock_quantity\n",
      "1\tLevi\tWhite\tXS\t22\t100\n",
      "2\tVan Huesen\tBlue\tL\t32\t52\n",
      "3\tNike\tRed\tXL\t10\t34\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "# MySQL connection parameters\n",
    "db_user = \"root\"\n",
    "db_password = \"root\"\n",
    "db_host = \"localhost\"\n",
    "db_name = \"atliq_tshirts\"\n",
    "\n",
    "# Create the SQLDatabase object\n",
    "db = SQLDatabase.from_uri(\n",
    "    f\"mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}\",\n",
    "    sample_rows_in_table_info=3  # optional: sample rows info\n",
    ")\n",
    "\n",
    "# Print table information\n",
    "print(db.table_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b0c86dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many t-shirts do we have left for nike in extra small size and white color?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT stock_quantity FROM t_shirts WHERE brand = 'Nike' AND color = 'White' AND size = 'XS'\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(91,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m91\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "qns1 = db_chain(\"How many t-shirts do we have left for nike in extra small size and white color?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d100193",
   "metadata": {},
   "source": [
    "Above is the correct answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b436818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How much is the price of the inventory for all small size t-shirts?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSQLQuery: SELECT sum(`price` * `stock_quantity`) FROM t_shirts WHERE `size` = 'S'\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(Decimal('23489'),)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe total price of the inventory for all small size t-shirts is 23489.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "qns2 = db_chain.run(\"How much is the price of the inventory for all small size t-shirts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632f6dc",
   "metadata": {},
   "source": [
    "It made a mistake here. The price is actually the price per unit but in real life database columns will not have perfect names. We need to tell it somehow that price is price per unit and the actual query should be,\n",
    "\n",
    "SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d613ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSQLQuery: SELECT SUM(`price` * `stock_quantity`) FROM t_shirts WHERE `size` = 'S'\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(Decimal('23489'),)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m23489\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "qns2 = db_chain.run(\"SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a4e2c",
   "metadata": {},
   "source": [
    "we will use qns2 value later on in this notebook. So hold on for now and let's check another query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da6c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "If we have to sell all the Levis T-shirts today with discounts applied. How much revenue our store will generate (post discounts)?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mQuestion: If we have to sell all the Levis T-shirts today with discounts applied. How much revenue our store will generate (post discounts)?\n",
      "SQLQuery: SELECT SUM(T1.price * (1 - COALESCE(T2.pct_discount, 0) / 100) * T1.stock_quantity) AS total_revenue FROM t_shirts AS T1 LEFT JOIN discounts AS T2 ON T1.t_shirt_id = T2.t_shirt_id WHERE T1.brand = 'Levi'\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(Decimal('29499.000000'),)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mQuestion: If we have to sell all the Levis T-shirts today with discounts applied. How much revenue our store will generate (post discounts)?\n",
      "SQLQuery:SELECT SUM(T1.price * (1 - COALESCE(T2.pct_discount, 0) / 100) * T1.stock_quantity) FROM t_shirts AS T1 LEFT JOIN discounts AS T2 ON T1.t_shirt_id = T2.t_shirt_id WHERE T1.brand = 'Levi'\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "qns3 = db_chain.run(\"If we have to sell all the Levis T-shirts today with discounts applied. How much revenue our store will generate (post discounts)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f02fd",
   "metadata": {},
   "source": [
    "Above, it returned a wrong query which generated an error during query execution. It thinks discount\n",
    "table would have start and end date which is normally true but in our table there is no start or end date column.\n",
    "One thing we can do here is run the query directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4b3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\n",
      "Please retry in 56.392863729s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "\n",
      "select sum(a.total_amount * ((100-COALESCE(discounts.pct_discount,0))/100)) as total_revenue from\n",
      "(select sum(price*stock_quantity) as total_amount, t_shirt_id from t_shirts where brand = 'Levi'\n",
      "group by t_shirt_id) a left join discounts on a.t_shirt_id = discounts.t_shirt_id\n",
      " \n",
      "SQLQuery:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 2\n",
      "Please retry in 41.199256516s. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 41\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mQuestion: select sum(a.total_amount * ((100-COALESCE(discounts.pct_discount,0))/100)) as total_revenue from\n",
      "(select sum(price*stock_quantity) as total_amount, t_shirt_id from t_shirts where brand = 'Levi'\n",
      "group by t_shirt_id) a left join discounts on a.t_shirt_id = discounts.t_shirt_id\n",
      "SQLQuery: SELECT SUM(a.total_amount * ((100 - COALESCE(d.pct_discount, 0)) / 100)) AS total_revenue FROM (SELECT SUM(`price` * `stock_quantity`) AS total_amount, `t_shirt_id` FROM t_shirts WHERE `brand` = 'Levi' GROUP BY `t_shirt_id`) AS a LEFT JOIN discounts AS d ON a.t_shirt_id = d.t_shirt_id\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(Decimal('29499.000000'),)]\u001b[0m\n",
      "Answer:"
     ]
    }
   ],
   "source": [
    "sql_code = \"\"\"\n",
    "select sum(a.total_amount * ((100-COALESCE(discounts.pct_discount,0))/100)) as total_revenue from\n",
    "(select sum(price*stock_quantity) as total_amount, t_shirt_id from t_shirts where brand = 'Levi'\n",
    "group by t_shirt_id) a left join discounts on a.t_shirt_id = discounts.t_shirt_id\n",
    " \"\"\"\n",
    "\n",
    "qns3 = db_chain.run(sql_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbf6ca",
   "metadata": {},
   "source": [
    "It produced a correct answer when explicit query was given. 17462 is the total revenue without discounts. The total discount is 736.6. Hence revenue post discount is 17462-736.6=16725.4\n",
    "\n",
    "Now this is not much interesting because what is the point of giving it the ready made query? Well, we will use this same query later on for few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ab6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qns4 = db_chain.run(\"SELECT SUM(price * stock_quantity) FROM t_shirts WHERE brand = 'Levi'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e822ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qns5 = db_chain.run(\"How many white color Levi's t shirts we have available?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee0e11",
   "metadata": {},
   "source": [
    "Once again above is the wrong answer. We need to use SUM(stock_quantity). Let's run the query explicitly. We will use the result of this query later on in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d15ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qns5 = db_chain.run(\"SELECT sum(stock_quantity) FROM t_shirts WHERE brand = 'Levi' AND color = 'White'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe780d9",
   "metadata": {},
   "source": [
    "#### Few shot learning\n",
    "\n",
    "We will use few shot learning to fix issues we have seen so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ae546",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shots = [\n",
    "    {'Question' : \"How many t-shirts do we have left for Nike in XS size and white color?\",\n",
    "     'SQLQuery' : \"SELECT sum(stock_quantity) FROM t_shirts WHERE brand = 'Nike' AND color = 'White' AND size = 'XS'\",\n",
    "     'SQLResult': \"Result of the SQL query\",\n",
    "     'Answer' : qns1},\n",
    "    {'Question': \"How much is the total price of the inventory for all S-size t-shirts?\",\n",
    "     'SQLQuery':\"SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'\",\n",
    "     'SQLResult': \"Result of the SQL query\",\n",
    "     'Answer': qns2},\n",
    "    {'Question': \"If we have to sell all the Levis T-shirts today with discounts applied. How much revenue  our store will generate (post discounts)?\" ,\n",
    "     'SQLQuery' : \"\"\"SELECT sum(a.total_amount * ((100-COALESCE(discounts.pct_discount,0))/100)) as total_revenue from\n",
    "(select sum(price*stock_quantity) as total_amount, t_shirt_id from t_shirts where brand = 'Levi'\n",
    "group by t_shirt_id) a left join discounts on a.t_shirt_id = discounts.t_shirt_id\n",
    " \"\"\",\n",
    "     'SQLResult': \"Result of the SQL query\",\n",
    "     'Answer': qns3} ,\n",
    "     {'Question' : \"If we have to sell all the Levis T-shirts today. How much revenue our store will generate without discount?\" ,\n",
    "      'SQLQuery': \"SELECT SUM(price * stock_quantity) FROM t_shirts WHERE brand = 'Levi'\",\n",
    "      'SQLResult': \"Result of the SQL query\",\n",
    "      'Answer' : qns4},\n",
    "    {'Question': \"How many white color Levi's shirt I have?\",\n",
    "     'SQLQuery' : \"SELECT sum(stock_quantity) FROM t_shirts WHERE brand = 'Levi' AND color = 'White'\",\n",
    "     'SQLResult': \"Result of the SQL query\",\n",
    "     'Answer' : qns5\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7b3c9",
   "metadata": {},
   "source": [
    "### Creating Semantic Similarity Based example selector\n",
    "\n",
    "- create embedding on the few_shots\n",
    "- Store the embeddings in Chroma DB\n",
    "- Retrieve the the top most Semantically close example from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54306b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in few_shots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95513027",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=few_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "example_selector.select_examples({\"Question\": \"How many Adidas T shirts I have left in my store?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### my sql based instruction prompt\n",
    "mysql_prompt = \"\"\"You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: Query to run with no pre-amble\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "No pre-amble.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ca60f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.chains.sql_database.prompt import PROMPT_SUFFIX, _mysql_prompt\n",
    "\n",
    "print(PROMPT_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20224496",
   "metadata": {},
   "source": [
    "### Setting up PromptTemplete using input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"Question\", \"SQLQuery\", \"SQLResult\",\"Answer\",],\n",
    "    template=\"\\nQuestion: {Question}\\nSQLQuery: {SQLQuery}\\nSQLResult: {SQLResult}\\nAnswer: {Answer}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_mysql_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=mysql_prompt,\n",
    "    suffix=PROMPT_SUFFIX,\n",
    "    input_variables=[\"input\", \"table_info\", \"top_k\"], #These variables are used in the prefix and suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, prompt=few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a32c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain(\"How many white color Levi's shirt I have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127786d",
   "metadata": {},
   "source": [
    "Now this is working ok. Previously for this same question it was giving wrong answer because it did not use SUM clause around stock_quantity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a05344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_chain(\"How much is the price of the inventory for all small size t-shirts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain(\"How much is the price of all white color levi t shirts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain(\"If we have to sell all the Nikes T-shirts today with discounts applied. How much revenue  our store will generate (post discounts)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain(\"If we have to sell all the Van Heuson T-shirts today with discounts applied. How much revenue  our store will generate (post discounts)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain.run('How much revenue  our store will generate by selling all Van Heuson TShirts without discount?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
